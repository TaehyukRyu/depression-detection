{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"private_outputs":true,"provenance":[],"gpuType":"T4","toc_visible":true},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"code","source":["from google.colab import drive\n","import pandas as pd\n","import json\n","\n","\n","# êµ¬ê¸€ ë“œë¼ì´ë¸Œ ë§ˆìš´íŠ¸\n","drive.mount('/content/drive')"],"metadata":{"id":"z71nKcUXQtsi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["SAVE_PATH='/content/drive/MyDrive/á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³/2025_á„€á…ªá„‹á…¬_á„‘á…³á„…á…©á„Œá…¦á†¨á„á…³1/data'"],"metadata":{"id":"siO_GQTUdJLJ"},"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{"id":"w4Mse4jBaDiD"},"outputs":[],"source":["data1 = pd.read_pickle(SAVE_PATH+'/ìŒì„±ë°ì´í„°ì „ì²˜ë¦¬_1.pkl')\n","data2 = pd.read_pickle(SAVE_PATH+'/ìŒì„±ë°ì´í„°ì „ì²˜ë¦¬_2.pkl')\n","data3 = pd.read_pickle(SAVE_PATH+'/ìŒì„±ë°ì´í„°ì „ì²˜ë¦¬_3.pkl')\n","data4 = pd.read_pickle(SAVE_PATH+'/ìŒì„±ë°ì´í„°ì „ì²˜ë¦¬_4.pkl')\n"]},{"cell_type":"code","source":["data=pd.concat([data1,data2])"],"metadata":{"id":"SX2jR93oRhgi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=pd.concat([data,data3])"],"metadata":{"id":"tGW9SVPyRqmm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=pd.concat([data,data4])"],"metadata":{"id":"aMEUuYQ-Rq4a"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# ì´ì œ ë°”ë¡œ ì‚¬ìš© ê°€ëŠ¥!\n","print(f\"âœ… MFCC íƒ€ì…: {type(data['mfcc'].iloc[0])}\")\n","print(f\"âœ… MFCC shape: {data['mfcc'].iloc[0].shape}\")"],"metadata":{"id":"EUSa4d5nSDG_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.isnull().sum()"],"metadata":{"id":"X1RLxQ64RuNo"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.info()"],"metadata":{"id":"PrGjcQd737zM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=data[['text','mfcc','label']]"],"metadata":{"id":"SlO1fNS0Ed_2"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data"],"metadata":{"id":"6o-4jnpoEnu4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for i in data['mfcc']:\n","    print(i.shape)\n","    break"],"metadata":{"id":"g-DGnkJiEI3h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['mfcc'][0].shape"],"metadata":{"id":"zOAUqtYyD8k3"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# **PHQ-9**\n"],"metadata":{"id":"2I4MFOcCHfVA"}},{"cell_type":"code","source":["direct_core = [\n","    \"ìì‚´ ìƒê°ì´ ë“ ë‹¤\", \"ì£½ê³ ì‹¶ë‹¤\", \"ì£½ëŠ”ê²Œ ë‚«ë‹¤\",\n","    \"ìí•´ ìƒê°ì´ ë“ ë‹¤\", \"ìí•´í•  ìƒê°ì´ë‹¤\"\n","]"],"metadata":{"id":"J-s7bjAEHzGd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["core_list = [\n","    \"ìš°ìš¸í•˜ë‹¤\", \"í¬ë§ì´ ì—†ë‹¤\", \"ì¦ê²ì§€ ì•Šë‹¤\",\n","    \"í–‰ë™ì´ ëŠë ¤ì¡Œë‹¤\", \"ë§ì´ ëŠë ¤ì¡Œë‹¤\",\n","    \"ì¼ìƒì— ì§‘ì¤‘ì„ ëª»í•œë‹¤\", \"ì‹¤íŒ¨í–ˆë‹¤\",\n","    \"ê°€ì¡±ì„ ì‹¤ë§ ì‹œì¼°ë‹¤\", \"ê¸°ìš´ì´ ì—†ë‹¤\", \"í”¼ê³¤í•˜ë‹¤\"\n","]\n"],"metadata":{"id":"ID-jTdJGH0AZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["indirect_list = [\n","    \"í¥ë¯¸ê°€ ì—†ë‹¤\", \"í¥ë¯¸ê°€ ë–¨ì–´ì§€ë‹¤\",\n","    \"ì‹ìš•ì´ ì¤„ë‹¤\", \"ì…ë§›ì´ ì—†ë‹¤\", \"ë§ì´ ë¨¹ë‹¤\",\n","    \"ì ë“¤ê¸° ì–´ë µë‹¤\", \"ì ì„ ë„ˆë¬´ ë§ì´ ì”ë‹¤\",\n","    \"ê°€ë§Œíˆ ìˆì§ˆ ëª»í•œë‹¤\", \"ì•ˆì ˆë¶€ì ˆí•˜ë‹¤\", \"ì˜ëª»í•˜ê³ ìˆë‹¤\"\n","]"],"metadata":{"id":"BRKFO0oJH1QF"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phq9_symptoms={'direct_core':direct_core,'core_list':core_list,'indirect_list':indirect_list }"],"metadata":{"id":"yDQmhpxuIF5P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phq9_keys = list(phq9_symptoms.keys())"],"metadata":{"id":"DEgB8O9vIrGP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phq9_keys"],"metadata":{"id":"sBQZH8BpIr9_"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **í‚¤ì›Œë“œ ì ìˆ˜ ê°€ì¤‘ì¹˜**\n","\n"," - phq-9 ìŠ¤ì½”ì–´ ì œì™¸"],"metadata":{"id":"PT8TPnADscih"}},{"cell_type":"markdown","source":["### í˜•íƒœì†Œ ë¶„ì„\n","    - phq9_symptoms ë‹¨ì–´ë“¤ê³¼ ì •ì œëœ í…ìŠ¤íŠ¸ ë°ì´í„°ë¥¼ ê°ê° í˜•íƒœì†Œ ë¶„ì„í•´ì¤€ë‹¤"],"metadata":{"id":"S3xJmf27sbn6"}},{"cell_type":"code","source":["# #Mecab ì„¤ì¹˜ (Colab)\n","# !pip install konlpy\n","# !pip install mecab-python3\n","# !bash <(curl -s https://raw.githubusercontent.com/konlpy/konlpy/master/scripts/mecab.sh)"],"metadata":{"id":"vaqNRBKfLZ9R","collapsed":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# from konlpy.tag import Mecab\n","\n","# # Mecab ê°ì²´ ìƒì„±\n","# mecab = Mecab()"],"metadata":{"collapsed":true,"id":"2qtrp8kQoLrn"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pos_phq9={}\n","\n","# for i in phq9_keys:\n","#     #print(phq9_symptoms[i])\n","\n","#     steam_list=[]\n","\n","#     for j in phq9_symptoms[i]:\n","#         #print(j)\n","#         pos=mecab.pos(j)\n","#         #print(pos)\n","\n","#         steam=[]\n","#         for word, tag in pos:\n","\n","#             #print(word, tag)\n","\n","#             if tag.startswith('VV') or tag.startswith('VA'): # VV(ë™ì‚¬), VA(í˜•ìš©ì‚¬)ì˜ ê²½ìš° ì–´ê°„ë§Œ ì‚¬ìš©\n","#                 steam.append(word)\n","#             elif tag.startswith('NN'):  # NNG(ì¼ë°˜ëª…ì‚¬), NNP(ê³ ìœ ëª…ì‚¬) ë“± ëª…ì‚¬ì˜ ê²½ìš° ë‹¨ì–´ ìì²´ë¥¼ ì‚¬ìš©\n","#                 steam.append(word)\n","\n","#             #print(steam)\n","\n","#         steam_list.append(' '.join(steam))\n","#         #print(steam_list)\n","\n","#     pos_phq9[i]=steam_list\n","\n","\n","# print(pos_phq9)"],"metadata":{"id":"FVPyj316pUkx"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# pos_phq9"],"metadata":{"id":"9zn6lIyjJWJw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# def pos(i):\n","\n","#     steam_list=[]\n","\n","#     pos=mecab.pos(i)\n","#     #print(pos)\n","\n","#     steam=[]\n","#     for word, tag in pos:\n","\n","#         #print(word, tag)\n","\n","#         if tag.startswith('VV') or tag.startswith('VA'): # VV(ë™ì‚¬), VA(í˜•ìš©ì‚¬)ì˜ ê²½ìš° ì–´ê°„ë§Œ ì‚¬ìš©\n","#             steam.append(word)\n","#         elif tag.startswith('NN'):  # NNG(ì¼ë°˜ëª…ì‚¬), NNP(ê³ ìœ ëª…ì‚¬) ë“± ëª…ì‚¬ì˜ ê²½ìš° ë‹¨ì–´ ìì²´ë¥¼ ì‚¬ìš©\n","#             steam.append(word)\n","\n","#         #print(steam)\n","\n","#     return ' '.join(steam)\n"],"metadata":{"id":"zsqP17FDwHQb"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data['text_pos']=data['text'].apply(lambda x: pos(x))"],"metadata":{"id":"V4hjjWrawGSk"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data"],"metadata":{"id":"jqruT5PxkGAg"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data['label'].value_counts()"],"metadata":{"id":"fbepis4JLO7U"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### phq9_score ìƒì„±"],"metadata":{"id":"unVF7yeSANUD"}},{"cell_type":"code","source":["# phq9_keys"],"metadata":{"id":"DDZwmjvMKqPI"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# import numpy as np\n","\n","# def create_phq9_vector(text):\n","#     phq9_secore=0\n","\n","#     for i,v in enumerate(phq9_keys):\n","\n","#         if v =='direct_core':\n","\n","#             for j in pos_phq9[v]:\n","\n","#                 if j in text:\n","#                     phq9_secore+=3.0\n","\n","\n","#         elif v =='core_list':\n","\n","#             for j in pos_phq9[v]:\n","\n","#                 if j in text:\n","#                     phq9_secore+=2.0\n","\n","\n","#         else:\n","\n","#             for j in pos_phq9[v]:\n","\n","#                 if j in text:\n","#                     phq9_secore+=1.0\n","\n","#     return  phq9_secore\n"],"metadata":{"id":"4yNmMljC0HTm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data['phq9_score']=data['text_pos'].apply(lambda x: create_phq9_vector(x))"],"metadata":{"id":"vMpQY9Fw9bzR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data"],"metadata":{"id":"B8gSJVNm9PRo"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ì˜ë¯¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜"],"metadata":{"id":"E4wzcYwAM9h3"}},{"cell_type":"markdown","source":["### í…ìŠ¤íŠ¸ ë°ì´í„° ë²¡í„°í™”"],"metadata":{"id":"Y1_jYkQI0l8H"}},{"cell_type":"code","source":["# max_length ì •í•˜ê¸°\n","\n","data['text_length']=data['text'].apply(lambda x: len(x))"],"metadata":{"id":"HjmEDhZwtQdM"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=32\n","\n","\n","max_length=int(data['text_length'].quantile(0.90))\n","print(f'90ì¸ ì§€ì :{max_length}')\n","print(f\"ìµœëŒ€ í…ìŠ¤íŠ¸ ê¸¸ì´: {data['text_length'].max()}\")\n","print(f\"í‰ê·  í…ìŠ¤íŠ¸ ê¸¸ì´: {data['text_length'].mean()}\")\n","\n"],"metadata":{"id":"t-PTkkRjkqWe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch\n","import torch.nn.functional as F\n","from transformers import AutoTokenizer, AutoModel\n","from tqdm import tqdm\n","import numpy as np\n","from tqdm import tqdm\n","\n","\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","print(f\"Using device: {device}\")\n","\n","MODEL_NAME = \"klue/roberta-base\"\n","tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME)\n","klue = AutoModel.from_pretrained(MODEL_NAME).to(device)"],"metadata":{"collapsed":true,"id":"-PjW_TEMOcmi"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def encode_texts(texts, max_length, batch_size):\n","\n","    if hasattr(texts, 'tolist'):\n","        texts = texts.tolist()\n","\n","    with torch.no_grad():\n","\n","        text_vecs=[]\n","        for i in tqdm(range(0, len(texts), batch_size)):\n","\n","            batch=texts[i:i+batch_size]\n","\n","            token=tokenizer(batch,\n","                    padding=True,  # ì§§ì€ ë¬¸ì¥ì€ íŒ¨ë”© ì¶”ê°€\n","                    truncation=True,  # ê¸´ ë¬¸ì¥ì€ ì˜ë¼ëƒ„\n","                    max_length=max_length,\n","                    return_tensors=\"pt\"  # íŒŒì´í† ì¹˜ í…ì„œë¡œ ë°˜í™˜\n","                )\n","\n","\n","            input_ids=token['input_ids'].to(device)\n","            attention_mask=token['attention_mask'].to(device)\n","\n","            output=klue.forward(input_ids, attention_mask )\n","            last_hidden_state=output.last_hidden_state\n","\n","\n","            mask=attention_mask.unsqueeze(-1).float() # íŒ¨ë”© ì œì™¸í•˜ê¸°\n","            mean=(last_hidden_state*mask).sum(dim=1)/mask.sum(dim=1).clamp(min=1e-9) # í‰ê·  ê³„ì‚°\n","            mean=F.normalize(mean, p=2, dim=1) # ë‹¨ìœ„ ë²¡í„°ë¡œ ë§Œë“¤ê¸°\n","\n","            text_vecs.append(mean.detach().cpu())\n","\n","        text_vecs=torch.cat(text_vecs, dim=0) # ë¦¬ìŠ¤íŠ¸=> í…ì„œí™”\n","\n","\n","    return text_vecs"],"metadata":{"id":"foH8HBuJqDDO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"ëª¨ë¸ ìµœëŒ€ ê¸¸ì´: {klue.config.max_position_embeddings}\") , print(f\"í˜„ì¬ max_length: {max_length}\")"],"metadata":{"id":"JI8bVJ7vHCbI"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### phq-9 ë‹¨ì–´ ë²¡í„°í™”"],"metadata":{"id":"ERzprzxQ3Cf-"}},{"cell_type":"code","source":["phq9_words=[]\n","\n","for i in phq9_symptoms:\n","    for v in phq9_symptoms[i]:\n","        phq9_words.append(v)"],"metadata":{"collapsed":true,"id":"b9wlGc2P3KyC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["phq9_emb=encode_texts(phq9_words, len(phq9_words), batch_size)"],"metadata":{"id":"yPyDj52P3KvZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if max_length > 512:\n","    max_length=512\n","else:\n","    pass"],"metadata":{"id":"5t5ok6ZcHjZa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_emb = encode_texts(data['text'], max_length, batch_size)"],"metadata":{"id":"xhZnHQP4-fTM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ë‚´ì "],"metadata":{"id":"hYUGNze8_BZ7"}},{"cell_type":"code","source":["cos_sim=(text_emb @ phq9_emb.T).numpy()\n","\n","max_sim=cos_sim.max(axis=1)"],"metadata":{"id":"NyIeI_3F_DzT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['similarity']=max_sim"],"metadata":{"id":"1VWmg_uDC1Hj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"id":"jJ9xtczNC8aH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## ìµœì¢… ì ìˆ˜ ê³„ì‚°\n","- total_all = {(np.array(kw_scores_all) * W_KW) + (max_sim_all * W_SEM)} * TARGET_CATS\n","    - W_KW = 1.0 # í‚¤ì›Œë“œ ì ìˆ˜ ê°€ì¤‘ì¹˜ ìì‚´\n","    - W_SEM = 0.6 # ì˜ë¯¸ ìœ ì‚¬ë„ ê°€ì¤‘ì¹˜\n","    - TARGET_CATS = [\"í•™ì—… ë° ì§„ë¡œ\", \"í•™êµí­ë ¥/ë”°ëŒë¦¼\"] CATEGORY_BOOST = 1.5\n"],"metadata":{"id":"jN0tgOlANVAQ"}},{"cell_type":"code","source":["# W_KW=1.0\n","# W_SEM= 0.6"],"metadata":{"id":"KbNNOe8iDV-H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#concat_data['TARGET_CATS']=concat_data['ìƒí™©í‚¤ì›Œë“œ'].apply(lambda x: 1.5 if x in [\"í•™ì—… ë° ì§„ë¡œ\", \"í•™êµí­ë ¥/ë”°ëŒë¦¼\"] else 1.0)"],"metadata":{"id":"2lfSLRCpD6XO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data['phq9_total_score']=((data['phq9_score']*W_KW)+(data['similarity']*W_SEM))"],"metadata":{"id":"YUQEpNFGDASC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data.columns"],"metadata":{"id":"w_QVGPKQEg-h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data.head()"],"metadata":{"id":"kS5t-k4LMVtW"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data=data[['text','similarity','mfcc','label']]"],"metadata":{"id":"NNs0tzJdEj05"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['label'].value_counts()"],"metadata":{"id":"O_F0yY9YE28f"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data.head()"],"metadata":{"id":"IDtBURixQvnn"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# mfcc(max length) íŒ¨ë”©"],"metadata":{"id":"XItQC_dHe18Q"}},{"cell_type":"code","source":["# ë¼ì´ë¸ŒëŸ¬ë¦¬ ì„í¬íŠ¸\n","import numpy as np\n","import pandas as pd\n","from sklearn.preprocessing import LabelEncoder\n","from sklearn.model_selection import train_test_split\n","from imblearn.over_sampling import SMOTE\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","from torch.utils.data import TensorDataset, DataLoader\n","from tqdm import tqdm\n","import matplotlib.pyplot as plt"],"metadata":{"id":"F6wjNwfXMr17"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["## mfcc ìµœëŒ€ ê¸¸ì´\n","time_steps = [arr.shape[1] for arr in data['mfcc']]\n","max_time = max(time_steps)\n","percentile_90 = int(np.percentile(time_steps, 90))\n","mean_time = np.mean(time_steps)\n","\n","print(f\"ìµœëŒ€ ê¸¸ì´: {max_time}\")\n","print(f\"90% ë¶„ìœ„ìˆ˜: {percentile_90}\")\n","print(f\"í‰ê·  ê¸¸ì´: {mean_time:.1f}\")\n","\n","# 90% ë¶„ìœ„ìˆ˜ ì‚¬ìš©\n","MAX_TIME_STEPS = percentile_90\n"],"metadata":{"id":"ewx-tetsfK6h"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 2. íŒ¨ë”© í•¨ìˆ˜\n","def pad_mfcc(mfcc_array, max_length):\n","\n","    current_length = mfcc_array.shape[1]\n","\n","    if current_length < max_length:\n","        # Zero padding\n","        pad_width = ((0, 0), (0, max_length - current_length))\n","        return np.pad(mfcc_array, pad_width, mode='constant', constant_values=0)\n","    else:\n","        # Truncate\n","        return mfcc_array[:, :max_length]"],"metadata":{"id":"ndQqseZQOVU5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['mfcc_padded'] = data['mfcc'].apply(lambda x: pad_mfcc(x, MAX_TIME_STEPS))"],"metadata":{"id":"zhb4pX1jgJey"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ëª¨ë¸ë§"],"metadata":{"id":"9pKojVYMMvPb"}},{"cell_type":"markdown","source":["## **multimodal**"],"metadata":{"id":"jwF4ZrrongsO"}},{"cell_type":"code","source":["from transformers import AutoTokenizer\n","import numpy as np"],"metadata":{"id":"5U2PX_Ue6INZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["data['text']"],"metadata":{"id":"lRM49SN4hMTY"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Klue í† í¬ë‚˜ì´ì € ë¡œë“œ\n","tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')"],"metadata":{"id":"OOOXwHoU9DbN"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["all_text_token_length=[]"],"metadata":{"id":"SYavrFns6ILE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["for text in data['text']:\n","    encoding=tokenizer(text, add_special_tokens=True)\n","    all_text_token_length.append(len(encoding['input_ids']))\n","\n","print(f'ì´ {len(all_text_token_length)}ê°œì˜ ë¬¸ì¥ì— ëŒ€í•œ í† í° ê¸¸ì´ ê³„ì‚° ì™„ë£Œ')\n","print(f'ì˜ˆì‹œ ì• 5ê°œ ë¬¸ì¥ê¸¸ì´ {all_text_token_length[:5]}')"],"metadata":{"id":"81uLaNBl-Ok9"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print('ëª¨ë“  ë¬¸ì¥ì—ì„œ')\n","print(f'í† í° ê¸¸ì´ ìµœëŒ“ê°’ì€ {max(all_text_token_length)} ì´ë‹¤.')\n","print(f'í† í° ê¸¸ì´ ìµœì†Ÿê°’ì€ {min(all_text_token_length)} ì´ë‹¤.')\n","\n","print(f'í† í° ê¸¸ì´ í‰ê· ì€ {np.mean(all_text_token_length)} ì´ë‹¤.')\n","print(f'í† í° ê¸¸ì´ í‘œì¤€í¸ì°¨ëŠ” {np.std(all_text_token_length)} ì´ë‹¤.')\n","\n","\n","print(\"-\" * 30)\n","print(f\"90% ì§€ì ì˜ ê¸¸ì´: {np.percentile(all_text_token_length, 90)}\")\n","print(f\"95% ì§€ì ì˜ ê¸¸ì´: {np.percentile(all_text_token_length, 95)}\")\n","print(f\"99% ì§€ì ì˜ ê¸¸ì´: {np.percentile(all_text_token_length, 99)}\")"],"metadata":{"id":"aXR1zbj9hDox"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_max_length=890.25"],"metadata":{"collapsed":true,"id":"tYRT5EZTh-Rd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["if text_max_length>512:\n","    text_max_length=512"],"metadata":{"id":"MXRoyzsfP-qP"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["text_max_length"],"metadata":{"id":"NJrEZfDlQrIe"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ë°ì´í„°ë¡œë“œ í•¨ìˆ˜"],"metadata":{"id":"740YKW6Tn9JB"}},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","import random\n","import copy\n","from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","\n","batch_size=32"],"metadata":{"id":"vKh6bqNBn699"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multimodal_data_load(text, max_length):\n","\n","    tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n","\n","    label_encoder = LabelEncoder()\n","    label_encoded = label_encoder.fit_transform(text['label'])\n","    text = text.copy()\n","    text['label_encoded'] = label_encoded\n","\n","    label_count = text['label'].nunique()\n","\n","    print(f\"í´ë˜ìŠ¤ ìˆ˜: {label_count}\")\n","    for idx, label_name in enumerate(label_encoder.classes_):\n","        print(f\"  {idx}: {label_name}\")\n","\n","\n","    train_idx, test_idx = train_test_split(\n","        np.arange(len(text)),\n","        test_size=0.3,\n","        stratify=text['label_encoded'],\n","        random_state=42\n","    )\n","\n","    train_data = text.iloc[train_idx].reset_index(drop=True)\n","    test_data = text.iloc[test_idx].reset_index(drop=True)\n","\n","\n","\n","\n","\n","\n","\n","    # ============================================================\n","    # ğŸ”¥ [í•µì‹¬] WeightedRandomSampler ì„¤ì •\n","    # ë°ì´í„°í”„ë ˆì„ì„ ëŠ˜ë¦¬ì§€ ì•Šê³ , ë¶ˆê· í˜•í•œ ë°ì´í„°ë¥¼ ê· í˜• ì¡íˆê²Œ ë½‘ì•„ì¤ë‹ˆë‹¤.\n","    # ============================================================\n","\n","    # 1. í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°\n","    class_counts = train_data['label_encoded'].value_counts().sort_index()\n","\n","    # 2. í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê°œìˆ˜ê°€ ì ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ê°€ ë†’ìŒ)\n","    # weight = 1 / count\n","    class_weights = 1.0 / class_counts\n","    class_weights = torch.tensor(class_weights.values, dtype=torch.float)\n","\n","    # 3. ê° ìƒ˜í”Œ(Row)ë§ˆë‹¤ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n","    # ì˜ˆ: ìš°ìš¸ì¦ ë°ì´í„°ëŠ” ê°€ì¤‘ì¹˜ 10, ì¼ë°˜ ë°ì´í„°ëŠ” ê°€ì¤‘ì¹˜ 1\n","    sample_weights = [class_weights[label] for label in train_data['label_encoded']]\n","    sample_weights = torch.tensor(sample_weights, dtype=torch.float)\n","\n","    # 4. ìƒ˜í”ŒëŸ¬ ìƒì„±\n","    # num_samplesë¥¼ ì§€ì •í•˜ì—¬ ì „ì²´ ë°ì´í„° í¬ê¸°ë¥¼ ì¡°ì ˆí•  ìˆ˜ ìˆìŒ (ì—¬ê¸°ì„  ì›ë³¸ í¬ê¸° ìœ ì§€í•˜ê±°ë‚˜ ëŠ˜ë¦´ ìˆ˜ ìˆìŒ)\n","    # ë³´í†µ ë¶ˆê· í˜• í•´ì†Œë¥¼ ìœ„í•´ ì¶©ë¶„íˆ ë½‘ê¸° ìœ„í•´ len(train_data) ê·¸ëŒ€ë¡œ ì‚¬ìš©í•˜ê±°ë‚˜ ëŠ˜ë¦¼.\n","    sampler = WeightedRandomSampler(\n","        weights=sample_weights,\n","        num_samples=len(train_data), # í˜¹ì€ len(train_data) * 2 ë“±ìœ¼ë¡œ ëŠ˜ë ¤ë„ RAM ì•ˆ í„°ì§\n","        replacement=True\n","    )\n","    # ============================================================\n","\n","\n","\n","\n","\n","\n","    # Dataset ìƒì„±\n","    train_dataset = TextDataset(\n","        texts=train_data['text'],\n","        phq9_features=train_data[['similarity']],\n","        mfcc=train_data['mfcc_padded'],\n","        labels=train_data['label_encoded'],\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        augment=True\n","    )\n","\n","    test_dataset = TextDataset(\n","        texts=test_data['text'],\n","        phq9_features=test_data[['similarity']],\n","        mfcc=test_data['mfcc_padded'],\n","        labels=test_data['label_encoded'],\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        augment=False\n","    )\n","\n","\n","\n","    # DataLoader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size,sampler=sampler, shuffle=False)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ\")\n","\n","    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ\")\n","\n","    return train_loader, test_loader, label_count"],"metadata":{"id":"vbm9wMrdngsO"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class TextDataset(Dataset):\n","    def __init__(self, texts, phq9_features, mfcc, labels, tokenizer, max_length, augment=False):\n","        self.texts = texts.reset_index(drop=True)   # í™•ì‹¤í•˜ê²Œ ë¦¬ì…‹\n","        self.labels = labels.reset_index(drop=True) # í™•ì‹¤í•˜ê²Œ ë¦¬ì…‹\n","        self.tokenizer=tokenizer\n","        self.max_length=int(max_length)\n","        self.phq9_features=phq9_features.reset_index(drop=True)\n","        self.mfcc = mfcc.reset_index(drop=True)\n","\n","        self.augment = augment #ì¦ê°•ì²˜ë¦¬\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","\n","\n","    # --- ë‚´ë¶€ ì¦ê°• í•¨ìˆ˜ ---\n","    def augment_mfcc(self, mfcc_data):\n","        aug_mfcc = copy.deepcopy(mfcc_data)\n","        noise = np.random.normal(0, 0.02, aug_mfcc.shape)\n","        return aug_mfcc + noise\n","\n","    def augment_text(self, text):\n","        words = text.split()\n","        if len(words) <= 1: return text\n","        if random.random() > 0.5:\n","            idx1, idx2 = random.sample(range(len(words)), 2)\n","            words[idx1], words[idx2] = words[idx2], words[idx1]\n","        else:\n","            if len(words) > 2:\n","                del words[random.randint(0, len(words)-1)]\n","        return ' '.join(words)\n","    # --------------------\n","\n","\n","\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts.iloc[idx])\n","        phq9_features=self.phq9_features.iloc[idx].values.astype(np.float32)\n","        label = int(self.labels.iloc[idx])\n","\n","        mfcc_data = self.mfcc.iloc[idx]\n","\n","\n","        if self.augment:\n","          text = self.augment_text(text)\n","          mfcc_data = self.augment_mfcc(mfcc_data)\n","\n","\n","\n","\n","\n","        mfcc_data = np.expand_dims(mfcc_data, axis=0)\n","\n","\n","        encoding=self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","             )\n","        return (\n","                encoding['input_ids'].flatten(),\n","                encoding['attention_mask'].flatten(),\n","                torch.tensor(phq9_features, dtype=torch.float32),\n","                torch.tensor(mfcc_data, dtype=torch.float32),\n","                torch.tensor(label, dtype=torch.long)\n","        )"],"metadata":{"id":"wam6OvaEpeQm"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ëª¨ë¸ í•¨ìˆ˜"],"metadata":{"id":"xbVw8FjwngsQ"}},{"cell_type":"code","source":["from transformers import AutoModel,AutoConfig\n","import torch.nn as nn\n","from tqdm import tqdm\n"],"metadata":{"id":"_HLWOwdPngsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class LSTMAudioClassifier(nn.Module):\n","    def __init__(self, num_class, input_size=30, hidden_size=128, num_layers=2, dropout=0.3, target_length=500):\n","        super(LSTMAudioClassifier, self).__init__()\n","\n","        self.target_length = target_length\n","\n","        self.lstm = nn.LSTM(\n","            input_size=input_size,\n","            hidden_size=hidden_size,\n","            num_layers=num_layers,\n","            batch_first=True,\n","            dropout=dropout,\n","            bidirectional=True\n","        )\n","        self.dropout = nn.Dropout(dropout)\n","        self.classifier = nn.Linear(hidden_size*2, num_class)\n","\n","    def forward(self, mfcc):\n","        # mfcc shape: (batch, 1, 30, 8144)\n","        x = mfcc.squeeze(1)      # (batch, 30, 8144)\n","\n","\n","        # Adaptive Average Poolingìœ¼ë¡œ ì‹œí€€ìŠ¤ ê¸¸ì´ ì¶•ì†Œ\n","        x = F.adaptive_avg_pool1d(x, output_size=self.target_length)\n","        x = x.permute(0, 2, 1)\n","\n","        lstm_out, _ = self.lstm(x)\n","        last_output = lstm_out[:, -1, :]\n","\n","        x = self.dropout(last_output)\n","        y_pred = self.classifier(x)\n","\n","        return y_pred"],"metadata":{"id":"bnf12IahWsUT"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class klueClassifier(nn.Module):\n","    def __init__(self, num_class, num_layers_to_train=3, dropout=0.3):\n","        super(klueClassifier,self).__init__()\n","\n","        self.klue=AutoModel.from_pretrained('klue/roberta-base')\n","        config= AutoConfig.from_pretrained('klue/roberta-base')\n","        hidden_size=config.hidden_size\n","\n","        # ============================================\n","        # ë¶€ë¶„ íŒŒì¸íŠœë‹\n","        total_layers = 12  # klueëŠ” 12ê°œ ë ˆì´ì–´\n","        layers_to_freeze = total_layers - num_layers_to_train\n","\n","        for param in self.klue.embeddings.parameters():\n","            param.requires_grad = False\n","\n","        # 2) ì²˜ìŒ Nê°œ ë ˆì´ì–´ freeze\n","        for i in range(layers_to_freeze):\n","            for param in self.klue.encoder.layer[i].parameters():\n","                param.requires_grad = False\n","        # ============================================\n","\n","\n","        self.phq9_feature_layer=nn.Linear(1,16)\n","\n","        self.dropout=nn.Dropout(dropout)\n","        self.classifier= nn.Linear(hidden_size+16, num_class)\n","\n","\n","\n","\n","    def forward(self, input_ids, attention_mask, phq9_features):\n","\n","        output=self.klue(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_token=output.pooler_output\n","\n","\n","        phq9=self.phq9_feature_layer(phq9_features)\n","        phq9=torch.relu(phq9)\n","        combined =torch.cat([cls_token, phq9], dim=1)\n","\n","\n","        x=self.dropout(combined)\n","        y_pred=self.classifier(x)\n","\n","\n","        return y_pred\n","\n"],"metadata":{"id":"DCZ0WZ_2ngsR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class MultiModalClassifier(nn.Module):\n","    def __init__(self, num_class, text_num_layers_to_train=3, audio_input_size=30, audio_hidden_size=128, audio_target_length=500):\n","        super(MultiModalClassifier, self).__init__()\n","\n","        # í…ìŠ¤íŠ¸ + PHQ9 ëª¨ë¸\n","        self.text_model = klueClassifier(\n","            num_class=num_class,\n","            num_layers_to_train=text_num_layers_to_train\n","        )\n","\n","        # ìŒì„±(MFCC) ëª¨ë¸\n","        self.audio_model = LSTMAudioClassifier(\n","            num_class=num_class,\n","            input_size=audio_input_size,\n","            hidden_size=audio_hidden_size,\n","            target_length=audio_target_length\n","        )\n","\n","    def forward(self, input_ids, attention_mask, phq9_features, mfcc):\n","        text_y = self.text_model(input_ids, attention_mask, phq9_features)\n","        audio_y = self.audio_model(mfcc)\n","\n","        final_y = (text_y + audio_y) / 2.0\n","\n","        return final_y"],"metadata":{"id":"fd26bbumZlXd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def multimodal_train(train_loader, model, criterion, optimizer, device, epochs=6):\n","\n","    history = {'Epoch': [], 'loss': []}\n","\n","    for i in range(epochs):\n","        model.train()\n","        total_loss=0\n","\n","        for x, attention_mask, phq9, mfcc, y in tqdm(train_loader):\n","\n","            x= x.to(device)\n","            attention_mask=attention_mask.to(device)\n","            phq9=phq9.to(device)\n","            mfcc=mfcc.to(device)\n","            y= y.to(device)\n","\n","            y_pred=model(x, attention_mask, phq9, mfcc)\n","            loss=criterion(y_pred, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            total_loss+=loss.item()\n","\n","\n","        history['loss'].append(total_loss / len(train_loader))\n","        history['Epoch'].append(i)\n","\n","        if i % 2 == 0:\n","            print (f'Epoch {i}: Loss={total_loss/len(train_loader)}')\n","\n","    return history\n"],"metadata":{"collapsed":true,"id":"LQFkLjqVngsS"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ëª¨ë¸í‰ê°€ í•¨ìˆ˜"],"metadata":{"id":"SbIhuuaps6yG"}},{"cell_type":"code","source":["from sklearn.metrics import(\n","    accuracy_score,\n","    f1_score,\n","    precision_score,\n","    recall_score,\n","    classification_report,\n","    confusion_matrix\n",")\n","import numpy as np\n","\n"],"metadata":{"id":"Fs2xAjqXs4t8"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def multimodal_eval(test_loader, model, depression_classes, device=device, label_names=None ):\n","    model.eval()\n","\n","    all_preds=[]\n","    all_labels=[]\n","\n","    with torch.no_grad():\n","        for x, attention_mask, phq9, mfcc, y in test_loader:  # âœ… 3ê°œ!\n","            x = x.to(device)\n","            attention_mask = attention_mask.to(device)\n","            phq9=phq9.to(device)\n","            mfcc=mfcc.to(device)\n","\n","            y_pred = model(x, attention_mask, phq9, mfcc)  # âœ… attention_mask í¬í•¨\n","            preds = torch.argmax(y_pred, dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(y.cpu().numpy())\n","\n","\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    f1_macro = f1_score(all_labels, all_preds, average='macro')\n","    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n","    print(f\"âœ… Macro F1-Score: {f1_macro:.4f}\")\n","    print(f\"âœ… Weighted F1-Score: {f1_weighted:.4f}\")\n","    print(f\"\\nğŸ”¢ Confusion Matrix:\")\n","    print(cm)\n","\n","\n","\n","    # ============================================\n","    # 3. ìš°ìš¸ ì‹ í˜¸ ì§‘ì¤‘ ë¶„ì„\n","    # ============================================\n","\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(f\"ğŸ¯ ìš°ìš¸ ì‹ í˜¸ ì§‘ì¤‘ ë¶„ì„ ({depression_classes})\")\n","    print(\"=\"*60)\n","\n","\n","    # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜\n","    all_labels_np = np.array(all_labels)\n","    all_preds_np = np.array(all_preds)\n","\n","    y_true_binary = np.isin(all_labels_np, depression_classes).astype(int)\n","    y_pred_binary = np.isin(all_preds_np, depression_classes).astype(int)\n","\n","    # Precision & Recall\n","    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n","    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n","    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n","\n","\n","\n","    # ì‹¤ì œ ìš°ìš¸ ì‹ í˜¸ì¸ë° ê¸°íƒ€ë¡œ ì˜ˆì¸¡\n","    false_negatives = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n","    # True Positive\n","    true_positives = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n","    # ì „ì²´ ìš°ìš¸ ì‹ í˜¸\n","    total_depression = np.sum(y_true_binary == 1)\n","\n","\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"ğŸ¯ ìš°ìš¸ ì‹ í˜¸ ì§‘ì¤‘ ë¶„ì„ (ë¶ˆì•ˆ + ìƒì²˜ + ìŠ¬í””)\")\n","    print(f\"{'='*60}\")\n","    print(f\"âœ… Precision: {precision:.4f}\")\n","    print(f\"âœ… Recall: {recall:.4f}\")\n","    print(f\"âœ… F1-Score: {f1:.4f}\")\n","    print(f\"\\nâš ï¸ False Negative: {false_negatives}ê±´ / {total_depression}ê±´\")\n","    print(f\"âœ… True Positive: {true_positives}ê±´ / {total_depression}ê±´\")\n","\n","\n","    # Classification Report\n","    if label_names: # = ['ê¸°ì¨', 'ë‹¹í™©', 'ë¶„ë…¸', 'ë¶ˆì•ˆ', 'ìƒì²˜', 'ìŠ¬í””']\n","        print(f\"\\nğŸ“‹ Classification Report:\")\n","        print(classification_report(all_labels, all_preds, target_names=label_names))\n","\n","\n","\n","    return {\n","        'accuracy': accuracy,\n","        'f1_macro': f1_macro,\n","        'f1_weighted': f1_weighted,\n","        'confusion_matrix': cm,\n","        'depression_precision': precision,\n","        'depression_recall': recall,\n","        'depression_f1': f1,\n","        'false_negatives': false_negatives,\n","        'true_positives': true_positives,\n","        'total_depression': total_depression,\n","    }\n"],"metadata":{"id":"EISn4xjTs4t9"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ë°ì´í„° ì…ë ¥"],"metadata":{"id":"kPK9O1nFe8-U"}},{"cell_type":"code","source":["total_result=[]"],"metadata":{"id":"lJniRUgOG07D"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results={'task':'six_label_all_text_phq9_multimodal',\n","         'text': 'all',\n","         'label':'six',\n","         'model':'multimodal'\n","                    }"],"metadata":{"id":"BKrDCgkrDaLw"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["max_length=text_max_length\n","\n","batch_size=8 # ë©”ëª¨ë¦¬ ë¬¸ì œ\n","epochs=6"],"metadata":{"id":"JMb5Q5BwCPOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n","\n","train_loader, test_loader, label_count=multimodal_data_load(data, max_length)"],"metadata":{"id":"0JOJkKOkCPOG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import torch.nn.functional as F\n","\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","MultiModal_model = MultiModalClassifier(\n","    num_class=label_count,\n","    text_num_layers_to_train=3,\n","    audio_input_size=30,\n","    audio_hidden_size=128,\n","    audio_target_length=200\n",").to(device)\n","\n","MultiModal_criterion = nn.CrossEntropyLoss()\n","MultiModal_optimizer = torch.optim.AdamW(\n","    MultiModal_model.parameters(),\n","    lr=1e-5,\n","    weight_decay=0.01\n",")\n","\n","print(f\"âœ… ëª¨ë¸ ìƒì„± ì™„ë£Œ!\")"],"metadata":{"id":"zrPb-k6fCPOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["multimodal_train(train_loader, MultiModal_model, MultiModal_criterion, MultiModal_optimizer, device, epochs=6)"],"metadata":{"id":"QMFb7socCPOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(MultiModal_model.state_dict(), SAVE_PATH + 'phase3_six_label_all_text_phq9_multimodal.pt')\n","print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")"],"metadata":{"id":"zTrJylH3eAyE"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["six_label_depression_classes = [3, 4, 5]\n","four_label_depression_classes = [1]"],"metadata":{"id":"RJltx5MvCPOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result=multimodal_eval(test_loader, MultiModal_model, six_label_depression_classes)"],"metadata":{"id":"1uV_owolCPOH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results.update(result)"],"metadata":{"id":"3g3X9r4EE6g4"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_result.append(results)"],"metadata":{"id":"dVTT5cABGbg_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","del MultiModal_model\n","del MultiModal_optimizer\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"MGhlMRtxeYv2"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["## **Fine_Tuning**"],"metadata":{"id":"6me-6X3nwQtF"}},{"cell_type":"markdown","source":["### ë°ì´í„°ë¡œë“œ í•¨ìˆ˜"],"metadata":{"id":"qDiV_AnkwQtF"}},{"cell_type":"code","source":["max_length"],"metadata":{"id":"nQOlKc3ATtWd"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["from transformers import AutoModel, AutoTokenizer\n","from sklearn.preprocessing import LabelEncoder\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","\n","\n","batch_size=32"],"metadata":{"id":"NsTNieQUwQtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# data_phq9=data[['text','label','phq9_score','similarity','phq9_total_score']]\n","\n","data_phq9 = data[['text', 'label', 'similarity']]"],"metadata":{"id":"M3ebKq-dTx63"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","class TextDataset(Dataset):\n","    def __init__(self, texts, phq9_features, labels, tokenizer, max_length, augment=False):\n","        self.texts = texts.reset_index(drop=True)   # í™•ì‹¤í•˜ê²Œ ë¦¬ì…‹\n","        self.labels = labels.reset_index(drop=True) # í™•ì‹¤í•˜ê²Œ ë¦¬ì…‹\n","        self.tokenizer=tokenizer\n","        self.max_length=int(max_length)\n","        self.phq9_features=phq9_features.reset_index(drop=True)\n","        self.augment = augment\n","\n","    def __len__(self):\n","        return len(self.texts)\n","\n","\n","\n","    # --- ë‚´ë¶€ í…ìŠ¤íŠ¸ ì¦ê°• í•¨ìˆ˜ ---\n","    def augment_text(self, text):\n","        words = text.split()\n","        if len(words) <= 1: return text\n","\n","        # 50% í™•ë¥ ë¡œ ìˆœì„œ ì„ê¸°, 50% í™•ë¥ ë¡œ ë‹¨ì–´ ì‚­ì œ\n","        if random.random() > 0.5:\n","            idx1, idx2 = random.sample(range(len(words)), 2)\n","            words[idx1], words[idx2] = words[idx2], words[idx1]\n","        else:\n","            if len(words) > 2:\n","                del words[random.randint(0, len(words)-1)]\n","        return ' '.join(words)\n","    # ---------------------------\n","\n","\n","\n","    def __getitem__(self, idx):\n","        text = str(self.texts.iloc[idx])\n","\n","        # âœ… [í•µì‹¬] Train ëª¨ë“œì¼ ë•Œë§Œ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì¦ê°•\n","        if self.augment:\n","          text = self.augment_text(text)\n","\n","\n","\n","        phq9_features=self.phq9_features.iloc[idx].values.astype(np.float32)\n","        label = int(self.labels.iloc[idx])\n","\n","        encoding=self.tokenizer.encode_plus(\n","            text,\n","            add_special_tokens=True,\n","            max_length=self.max_length,\n","            padding='max_length',\n","            truncation=True,\n","            return_attention_mask=True,\n","            return_tensors='pt'\n","             )\n","        return (\n","                encoding['input_ids'].flatten(),\n","                encoding['attention_mask'].flatten(),\n","                torch.tensor(phq9_features, dtype=torch.float32),\n","                torch.tensor(label, dtype=torch.long)\n","        )"],"metadata":{"id":"hOQHxDWxwQtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def FineTuning_data_load(text, max_length):\n","\n","    tokenizer = AutoTokenizer.from_pretrained('klue/roberta-base')\n","\n","    label_encoder = LabelEncoder()\n","    label_encoded = label_encoder.fit_transform(text['label'])\n","    text = text.copy()\n","    text['label_encoded'] = label_encoded\n","\n","    label_count = text['label'].nunique()\n","\n","    print(f\"í´ë˜ìŠ¤ ìˆ˜: {label_count}\")\n","    for idx, label_name in enumerate(label_encoder.classes_):\n","        print(f\"  {idx}: {label_name}\")\n","\n","\n","    train_idx, test_idx = train_test_split(\n","        np.arange(len(text)),\n","        test_size=0.3,\n","        stratify=text['label_encoded'],\n","        random_state=42\n","    )\n","\n","    train_data = text.iloc[train_idx].reset_index(drop=True)\n","    test_data = text.iloc[test_idx].reset_index(drop=True)\n","\n","\n","\n","\n","\n","    print(\"âš ï¸ ë©”ëª¨ë¦¬ ì ˆì•½ì„ ìœ„í•´ WeightedRandomSampler(ì‹¤ì‹œê°„ ë°¸ëŸ°ì‹±)ë¥¼ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n","\n","    # ============================================================\n","    # ğŸ”¥ [í•µì‹¬] WeightedRandomSampler ì„¤ì • (ë°ì´í„° ë³µì‚¬ ì—†ì´ ë°¸ëŸ°ì‹±)\n","    # ============================================================\n","\n","    # 1. í´ë˜ìŠ¤ë³„ ìƒ˜í”Œ ìˆ˜ ê³„ì‚°\n","    class_counts = train_data['label_encoded'].value_counts().sort_index()\n","\n","    # 2. í´ë˜ìŠ¤ë³„ ê°€ì¤‘ì¹˜ ê³„ì‚° (ê°œìˆ˜ê°€ ì ì„ìˆ˜ë¡ ê°€ì¤‘ì¹˜ ë†’ìŒ)\n","    class_weights = 1.0 / class_counts\n","    class_weights = torch.tensor(class_weights.values, dtype=torch.float)\n","\n","    # 3. ê° ìƒ˜í”Œë§ˆë‹¤ ê°€ì¤‘ì¹˜ ë¶€ì—¬\n","    sample_weights = [class_weights[label] for label in train_data['label_encoded']]\n","    sample_weights = torch.tensor(sample_weights, dtype=torch.float)\n","\n","    # 4. ìƒ˜í”ŒëŸ¬ ìƒì„± (ë°ì´í„°ë¥¼ ëŠ˜ë¦¬ê³  ì‹¶ìœ¼ë©´ num_samplesë¥¼ len(train_data) * 2 ë“±ìœ¼ë¡œ ì„¤ì •)\n","    sampler = WeightedRandomSampler(\n","        weights=sample_weights,\n","        num_samples=len(train_data), # ì›ë³¸ ë°ì´í„° ê°œìˆ˜ ìœ ì§€ (ì›í•˜ë©´ ëŠ˜ë ¤ë„ ë¨)\n","        replacement=True\n","    )\n","    # ============================================================\n","\n","\n","\n","\n","\n","    # Dataset ìƒì„±\n","    train_dataset = TextDataset(\n","        texts=train_data['text'],\n","        phq9_features=train_data[['similarity']],\n","        labels=train_data['label_encoded'],\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        augment=True\n","    )\n","\n","    test_dataset = TextDataset(\n","        texts=test_data['text'],\n","        phq9_features=test_data[['similarity']],\n","        labels=test_data['label_encoded'],\n","        tokenizer=tokenizer,\n","        max_length=max_length,\n","        augment=False\n","    )\n","\n","    # DataLoader\n","    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, sampler=sampler)\n","    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n","\n","    print(f\"âœ… í›ˆë ¨ ë°ì´í„°: {len(train_dataset)}ê°œ\")\n","    print(f\"âœ… í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_dataset)}ê°œ\")\n","\n","    return train_loader, test_loader, label_count"],"metadata":{"id":"oNiw7CbXwQtG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ëª¨ë¸ í•¨ìˆ˜"],"metadata":{"id":"4g5xgkZDwQtG"}},{"cell_type":"code","source":["from transformers import AutoModel,AutoConfig\n","import torch.nn as nn\n","from tqdm import tqdm\n"],"metadata":{"id":"qrpeFQrqwQtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["class klueClassifier(nn.Module):\n","    def __init__(self, num_class, num_layers_to_train=3, dropout=0.3):\n","        super(klueClassifier,self).__init__()\n","\n","        self.klue=AutoModel.from_pretrained('klue/roberta-base')\n","        config= AutoConfig.from_pretrained('klue/roberta-base')\n","        hidden_size=config.hidden_size\n","\n","        # ============================================\n","        # ë¶€ë¶„ íŒŒì¸íŠœë‹\n","        total_layers = 12  # klueëŠ” 12ê°œ ë ˆì´ì–´\n","        layers_to_freeze = total_layers - num_layers_to_train\n","\n","        for param in self.klue.embeddings.parameters():\n","            param.requires_grad = False\n","\n","        # 2) ì²˜ìŒ Nê°œ ë ˆì´ì–´ freeze\n","        for i in range(layers_to_freeze):\n","            for param in self.klue.encoder.layer[i].parameters():\n","                param.requires_grad = False\n","        # ============================================\n","\n","\n","        self.phq9_feature_layer=nn.Linear(1,16)\n","\n","        self.dropout=nn.Dropout(dropout)\n","        self.classifier= nn.Linear(hidden_size+16, num_class)\n","\n","\n","\n","\n","    def forward(self, input_ids, attention_mask, phq9_features):\n","\n","        output=self.klue(input_ids=input_ids, attention_mask=attention_mask)\n","        cls_token=output.pooler_output\n","\n","\n","        phq9=self.phq9_feature_layer(phq9_features)\n","        phq9=torch.relu(phq9)\n","        combined =torch.cat([cls_token, phq9], dim=1)\n","\n","\n","        x=self.dropout(combined)\n","        y_pred=self.classifier(x)\n","\n","\n","        return y_pred\n","\n"],"metadata":{"id":"BbbAa-OGwQtG"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","def FineTuning_train(train_loader, model, criterion, optimizer, device=device, epochs=3):\n","\n","    history = {'Epoch': [], 'loss': []}\n","\n","    for i in range(epochs):\n","        model.train()\n","        total_loss=0\n","\n","        for x, attention_mask, phq9, y in tqdm(train_loader):\n","\n","            x= x.to(device)\n","            attention_mask=attention_mask.to(device)\n","            phq9=phq9.to(device)\n","            y= y.to(device)\n","\n","            y_pred=model(x, attention_mask, phq9)\n","            loss=criterion(y_pred, y)\n","\n","            optimizer.zero_grad()\n","            loss.backward()\n","\n","            optimizer.step()\n","\n","            total_loss+=loss.item()\n","\n","\n","        history['loss'].append(total_loss / len(train_loader))\n","        history['Epoch'].append(i)\n","\n","        if i % 2 == 0:\n","            print (f'Epoch {i}: Loss={total_loss/len(train_loader)}')\n","\n","    return history\n"],"metadata":{"collapsed":true,"id":"5bT3ZsrHwQtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ëª¨ë¸í‰ê°€ í•¨ìˆ˜"],"metadata":{"id":"fzfD--3pwQtH"}},{"cell_type":"code","source":["    from sklearn.metrics import(\n","        accuracy_score,\n","        f1_score,\n","        precision_score,\n","        recall_score,\n","        classification_report,\n","        confusion_matrix\n","    )\n","    import numpy as np\n","\n"],"metadata":{"id":"-NgirBvuwQtH"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["def FineTuning_eval(test_loader, model, depression_classes, device=device, label_names=None ):\n","    model.eval()\n","\n","    all_preds=[]\n","    all_labels=[]\n","\n","    with torch.no_grad():\n","        for x, attention_mask, phq9, y in test_loader:  # âœ… 3ê°œ!\n","            x = x.to(device)\n","            attention_mask = attention_mask.to(device)\n","            phq9=phq9.to(device)\n","\n","            y_pred = model(x, attention_mask, phq9)  # âœ… attention_mask í¬í•¨\n","            preds = torch.argmax(y_pred, dim=1)\n","\n","            all_preds.extend(preds.cpu().numpy())\n","            all_labels.extend(y.cpu().numpy())\n","\n","\n","\n","    accuracy = accuracy_score(all_labels, all_preds)\n","    f1_macro = f1_score(all_labels, all_preds, average='macro')\n","    f1_weighted = f1_score(all_labels, all_preds, average='weighted')\n","    cm = confusion_matrix(all_labels, all_preds)\n","\n","    print(f\"\\nâœ… Accuracy: {accuracy:.4f}\")\n","    print(f\"âœ… Macro F1-Score: {f1_macro:.4f}\")\n","    print(f\"âœ… Weighted F1-Score: {f1_weighted:.4f}\")\n","    print(f\"\\nğŸ”¢ Confusion Matrix:\")\n","    print(cm)\n","\n","\n","\n","    # ============================================\n","    # 3. ìš°ìš¸ ì‹ í˜¸ ì§‘ì¤‘ ë¶„ì„\n","    # ============================================\n","\n","\n","    print(\"\\n\" + \"=\"*60)\n","    print(f\"ğŸ¯ ìš°ìš¸ ì‹ í˜¸ ì§‘ì¤‘ ë¶„ì„ ({depression_classes})\")\n","    print(\"=\"*60)\n","\n","\n","    # ì´ì§„ ë¶„ë¥˜ë¡œ ë³€í™˜\n","    all_labels_np = np.array(all_labels)\n","    all_preds_np = np.array(all_preds)\n","\n","    y_true_binary = np.isin(all_labels_np, depression_classes).astype(int)\n","    y_pred_binary = np.isin(all_preds_np, depression_classes).astype(int)\n","\n","    # Precision & Recall\n","    precision = precision_score(y_true_binary, y_pred_binary, zero_division=0)\n","    recall = recall_score(y_true_binary, y_pred_binary, zero_division=0)\n","    f1 = f1_score(y_true_binary, y_pred_binary, zero_division=0)\n","\n","\n","\n","    # ì‹¤ì œ ìš°ìš¸ ì‹ í˜¸ì¸ë° ê¸°íƒ€ë¡œ ì˜ˆì¸¡\n","    false_negatives = np.sum((y_true_binary == 1) & (y_pred_binary == 0))\n","    # True Positive\n","    true_positives = np.sum((y_true_binary == 1) & (y_pred_binary == 1))\n","    # ì „ì²´ ìš°ìš¸ ì‹ í˜¸\n","    total_depression = np.sum(y_true_binary == 1)\n","\n","\n","\n","    print(f\"\\n{'='*60}\")\n","    print(\"ğŸ¯ ìš°ìš¸ ì‹ í˜¸ ì§‘ì¤‘ ë¶„ì„ (ë¶ˆì•ˆ + ìƒì²˜ + ìŠ¬í””)\")\n","    print(f\"{'='*60}\")\n","    print(f\"âœ… Precision: {precision:.4f}\")\n","    print(f\"âœ… Recall: {recall:.4f}\")\n","    print(f\"âœ… F1-Score: {f1:.4f}\")\n","    print(f\"\\nâš ï¸ False Negative: {false_negatives}ê±´ / {total_depression}ê±´\")\n","    print(f\"âœ… True Positive: {true_positives}ê±´ / {total_depression}ê±´\")\n","\n","\n","    # Classification Report\n","    if label_names: # = ['ê¸°ì¨', 'ë‹¹í™©', 'ë¶„ë…¸', 'ë¶ˆì•ˆ', 'ìƒì²˜', 'ìŠ¬í””']\n","        print(f\"\\nğŸ“‹ Classification Report:\")\n","        print(classification_report(all_labels, all_preds, target_names=label_names))\n","\n","\n","\n","    return {\n","        'accuracy': accuracy,\n","        'f1_macro': f1_macro,\n","        'f1_weighted': f1_weighted,\n","        'confusion_matrix': cm,\n","        'depression_precision': precision,\n","        'depression_recall': recall,\n","        'depression_f1': f1,\n","        'false_negatives': false_negatives,\n","        'true_positives': true_positives,\n","        'total_depression': total_depression,\n","    }\n"],"metadata":{"id":"Ivgjc8vPwQtH"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["### ë°ì´í„° ì…ë ¥"],"metadata":{"id":"G-xrJQv-fhPX"}},{"cell_type":"code","source":["phq9_total_result=[]\n","\n","\n","\n","results={'task':'six_label_all_text_phq9',\n","         'text': 'all',\n","         'label':'six',\n","         'model':'finetuning',\n","                    }"],"metadata":{"id":"TrygqJr5eWvC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["batch_size=8\n","epoch=6"],"metadata":{"id":"xGUKA4DUjn-P"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["six_label_depression_classes = [3, 4, 5]\n","four_label_depression_classes = [1]"],"metadata":{"id":"bZ_byFawVKvy"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["train_loader, test_loader, label_count=FineTuning_data_load(data_phq9, max_length)"],"metadata":{"id":"GPhMXChu7Rdh"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","finetuning_model = klueClassifier(num_class=label_count, num_layers_to_train=3, dropout=0.3).to(device)\n","\n","\n","FineTuning_criterion=nn.CrossEntropyLoss()\n","FineTuning_optimizer = torch.optim.AdamW([\n","    {'params': finetuning_model.klue.parameters(), 'lr': 2e-5},      # KLUEëŠ” ì‘ê²Œ\n","    {'params': finetuning_model.classifier.parameters(), 'lr': 1e-4}  # ë¶„ë¥˜ê¸°ëŠ” í¬ê²Œ\n","], weight_decay=0.01)"],"metadata":{"id":"fcp953QW7VHA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["FineTuning_train(train_loader, finetuning_model, FineTuning_criterion, FineTuning_optimizer, device, epochs=6)"],"metadata":{"id":"_jiJqWj77b7s"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["torch.save(finetuning_model.state_dict(), SAVE_PATH + 'phase3_six_label_all_text_phq9.pt')\n","print(\"ëª¨ë¸ ì €ì¥ ì™„ë£Œ\")"],"metadata":{"id":"Bfqp2PkneP4m"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["six_label_depression_classes = [3, 4, 5]\n","four_label_depression_classes = [1]"],"metadata":{"id":"1DHsOJjq75y5"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result=FineTuning_eval(test_loader, finetuning_model, six_label_depression_classes, device=device)"],"metadata":{"id":"ryN30PJ07giA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["results.update(result)"],"metadata":{"id":"lmWf5lwGwdgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_result.append(results)"],"metadata":{"id":"pxehAfZcwdgR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_result"],"metadata":{"id":"evWsIFGmeyY0"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import gc\n","del finetuning_model\n","del FineTuning_optimizer\n","torch.cuda.empty_cache()\n","gc.collect()"],"metadata":{"id":"gb6WjStheXbG"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["# ë¹„êµ ë° ì‹œê°í™”"],"metadata":{"id":"UeQJdpLmLzwx"}},{"cell_type":"code","source":["total_result=pd.DataFrame(total_result)"],"metadata":{"id":"HfqUPvBURGqa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_result.to_csv('total_result_phase3')"],"metadata":{"id":"eBD4koRPL2HA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["total_result"],"metadata":{"id":"9ny6oSZffbGe"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["import matplotlib.pyplot as plt\n","import seaborn as sns"],"metadata":{"id":"lazIuj5zNcDS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["figure, axs = plt.subplots(5,1,figsize=(10,20))\n","\n","metrics=['accuracy', 'f1_macro', 'depression_precision', 'depression_recall','depression_f1']\n","\n","for i,v in enumerate(metrics):\n","    #axs[i].bar(total_result['task'], total_result[v])\n","    sns.barplot(data=total_result, x='task', y=v, hue='model', ax= axs[i])\n","    axs[i].set_title(v, fontsize=18)\n","    axs[i].set_ylabel('Score')\n","    axs[i].grid(axis='y', alpha=0.3)\n","    axs[i].set_ylim(bottom=total_result[v].min() * 0.99)\n","\n","plt.tight_layout(h_pad=5.0)\n","\n","plt.show()"],"metadata":{"collapsed":true,"id":"9tk52221SknR"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"gzU1qP8ulnNX"},"execution_count":null,"outputs":[]}]}